# Model Tools

POMDPTools contains assorted tools that are not part of the core POMDPs.jl interface for working with (PO)MDP Models.

## Interface Extensions

POMDPTools contains several interface extensions that provide shortcuts and standardized ways of dealing with extra data.

Programmers should use these functions whenever possible in case optimized implementations are available, but all of the functions have default implementations based on the core POMDPs.jl interface.
Thus, if the core interface is implemented, all of these functions will also be available.

### Weighted Iteration

Many solution techniques, for example value iteration, require iteration through the support of a distribution and evaluating the probability mass for each value. In some cases, looking up the probability mass is expensive, so it is more efficient to iterate through value => probability pairs. `weighted_iterator` provides a standard interface for this.

```@docs
weighted_iterator
```

### Observation Weight

Sometimes, e.g. in particle filtering, the relative likelihood of an observation is required in addition to a generative model, and it is often tedious to implement a custom observation distribution type. For this case, the shortcut function `obs_weight` is provided.

```@docs
obs_weight
```

### Ordered Spaces

It is often useful to have a list of states, actions, or observations ordered consistently with the respective `index` function from POMDPs.jl. Since the POMDPs.jl interface does not demand that spaces be ordered consistently with `index`, the `states`, `actions`, and `observations` functions are not sufficient. Thus POMDPModelTools provides `ordered_actions`, `ordered_states`, and `ordered_observations` to provide this capability.

```@docs
ordered_actions
ordered_states
ordered_observations
```

### Info Interface

It is often the case that useful information besides the belief, state, action, etc is generated by a function in POMDPs.jl. This information can be useful for debugging or understanding the behavior of a solver, updater, or problem. The info interface provides a standard way for problems, policies, solvers or updaters to output this information. The recording simulators from [POMDPTools](@ref pomdptools_section) automatically record this information.

To specify info from policies, solvers, or updaters, implement the following functions:

```@docs
action_info
solve_info
update_info
```

## Model Transformations

POMDPTools contains several tools for transforming problems into other classes so that they can be used by different solvers.

### Linear Algebra Representations

For some algorithms, such as value iteration, it is convenient to use vectors that contain the reward for every state, and matrices that contain the transition probabilities. These can be constructed with the following functions:

```@docs
transition_matrices
reward_vectors
```

### Sparse Tabular MDPs and POMDPs

The `SparseTabularMDP` and `SparseTabularPOMDP` represents discrete problems defined using the explicit interface. The transition and observation models are represented using sparse matrices. Solver writers can leverage these data structures to write efficient vectorized code. A problem writer can define its problem using the explicit interface and it can be automatically converted to a sparse tabular representation by calling the constructors `SparseTabularMDP(::MDP)` or `SparseTabularPOMDP(::POMDP)`. See the following docs to know more about the matrix representation and how to access the fields of the `SparseTabular` objects:

```@docs
SparseTabularMDP
SparseTabularPOMDP
transition_matrix
reward_vector
observation_matrix
reward_matrix
observation_matrices
```

### Fully Observable POMDP

```@docs
FullyObservablePOMDP
```

### Generative Belief MDP

Every POMDP is an MDP on the belief space `GenerativeBeliefMDP` creates a generative model for that MDP.

!!! warning
    The reward generated by the `GenerativeBeliefMDP` is the reward for a *single state sampled from the belief*; it is not the   expected reward for that belief transition (though, in expectation, they are equivalent of course). Implementing the model with the expected reward requires a custom implementation because belief updaters do not typically deal with reward.

```@docs
GenerativeBeliefMDP
```

#### Example

```jldoctest belief_mdp; filter=r".*DiscreteBelief.*"
using POMDPs
using POMDPModels
using POMDPTools

pomdp = BabyPOMDP()
updater = DiscreteUpdater(pomdp)

belief_mdp = GenerativeBeliefMDP(pomdp, updater)
@show statetype(belief_mdp) # POMDPModels.BoolDistribution

for (a, r, sp) in stepthrough(belief_mdp, RandomPolicy(belief_mdp), "a,r,sp", max_steps=5)
    @show a, r, sp
end

# output
statetype(belief_mdp) = DiscreteBelief{POMDPModels.BabyPOMDP, Bool}Bool}
(a, r, sp) = (true, -5.0, DiscreteBelief{POMDPModels.BabyPOMDP, Bool}(POMDPModels.BabyPOMDP(-5.0, -10.0, 0.1, 0.8, 0.1, 0.9), Bool[0, 1], [1.0, 0.0]))
(a, r, sp) = (true, -5.0, DiscreteBelief{POMDPModels.BabyPOMDP, Bool}(POMDPModels.BabyPOMDP(-5.0, -10.0, 0.1, 0.8, 0.1, 0.9), Bool[0, 1], [1.0, 0.0]))
(a, r, sp) = (true, -5.0, DiscreteBelief{POMDPModels.BabyPOMDP, Bool}(POMDPModels.BabyPOMDP(-5.0, -10.0, 0.1, 0.8, 0.1, 0.9), Bool[0, 1], [1.0, 0.0]))
(a, r, sp) = (false, 0.0, DiscreteBelief{POMDPModels.BabyPOMDP, Bool}(POMDPModels.BabyPOMDP(-5.0, -10.0, 0.1, 0.8, 0.1, 0.9), Bool[0, 1], [0.9759036144578314, 0.02409638554216867]))
(a, r, sp) = (false, 0.0, DiscreteBelief{POMDPModels.BabyPOMDP, Bool}(POMDPModels.BabyPOMDP(-5.0, -10.0, 0.1, 0.8, 0.1, 0.9), Bool[0, 1], [0.9701315984030756, 0.029868401596924433]))
```

```@meta
DocTestSetup = nothing
```

### Underlying MDP

```@docs
UnderlyingMDP
```

### State Action Reward Model

```@docs
StateActionReward
```

## Utility Types

### Terminal State

`TerminalState` and its singleton instance `terminalstate` are available to use for a terminal state in concert with another state type. It has the appropriate type promotion logic to make its use with other types friendly, similar to `nothing` and `missing`.

!!! note

    NOTE: This is NOT a replacement for the standard POMDPs.jl isterminal function, though isterminal is implemented for the type. It is merely a convenient type to use for terminal states.

!!! warning
    
    WARNING: Early tests (August 2018) suggest that the Julia 1.0 compiler will not be able to efficiently implement union splitting in cases as  complex as POMDPs, so using a `Union` for the state type of a problem can currently have a large overhead.


```@docs
TerminalState
terminalstate
```
